
INFO:pytorch_lightning.strategies.deepspeed:initializing deepspeed distributed: GLOBAL_RANK: 8, MEMBER: 9/16
INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10155487060546875 seconds
10
Broadcasted tensor sizes list length: 902 rank: 8
Updated 902 keys on 8
Non zero tensors rank 8 - 0
tensor(11.2500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(9., device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)
tensor(9.1875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(8.7500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(8.5625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(8.3750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(8.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(7.9375, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(7.5938, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(7.5000, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(7.3750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(7.2500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(7., device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)
tensor(7.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(6.9375, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(6.9688, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(6.6875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(6.6875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(6.7500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<NllLossBackward0>)
tensor(6.7188, device='cuda:0', dtype=torch.bfloat16,

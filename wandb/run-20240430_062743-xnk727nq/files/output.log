
INFO:pytorch_lightning.strategies.deepspeed:initializing deepspeed distributed: GLOBAL_RANK: 8, MEMBER: 9/16
INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10152244567871094 seconds
Broadcasted tensor sizes list length: 902 rank: 8
Updated 902 keys on 8
Non zero tensors rank 8 - 0
tensor(401408., device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)
tensor(-1.9504e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-2.4432e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-2.7682e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-3.2506e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-3.8378e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-4.2362e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-4.7186e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-5.2639e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-5.4107e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-5.7881e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-5.8301e+08, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SumBackward0>)
tensor(-6.3753e+08, device='cuda:0', dtype=torch.bfloat16,
